{"posts":[{"no":122343469,"now":"04\/22\/17(Sat)19:15:37","name":"Anonymous","sub":"She&#039;s not wrong.","com":"<span class=\"quote\">&gt;Can machines think \u2013 and, if so, can they think critically about race and gender? Recent reports have shown that machine-learning systems are picking up racist and sexist ideas embedded in the language patterns they are fed by human engineers. The idea that machines can be as bigoted as people is an uncomfortable one for anyone who still believes in the moral purity of the digital future, but there\u2019s nothing new or complicated about it. \u201cMachine learning\u201d is a fancy way of saying \u201cfinding patterns in data\u201d. Of course, as Lydia Nicholas, senior researcher at the innovation thinktank Nesta, explains, all this data \u201chas to have been collected in the past, and since society changes, you can end up with patterns that reflect the past. If those patterns are used to make decisions that affect people\u2019s lives you end up with unacceptable discrimination.\u201d<\/span><br><br><span class=\"quote\">&gt;Robots have been racist and sexist for as long as the people who created them have been racist and sexist, because machines can work only from the information given to them, usually by the white, straight men who dominate the fields of technology and robotics. As long ago as 1986, the medical school at St George\u2019s hospital in London was found guilty of racial and sexual discrimination when it automated its admissions process based on data collected in the 1970s. The program looked at the sort of candidates who had been successful in the past, and gave similar people interviews. Unsurprisingly, the people the computer considered suitable were male, and had names that looked Anglo-Saxon.<\/span>","filename":"IMG_1831","ext":".jpg","w":750,"h":1165,"tn_w":160,"tn_h":250,"tim":1492902937876,"time":1492902937,"md5":"oMnd2+9npwg3UkPRdulI5A==","fsize":203721,"resto":0,"id":"ZmgSzrle","country":"US","m_img":1,"bumplimit":0,"imagelimit":0,"semantic_url":"shes-not-wrong","country_name":"United States","replies":8,"images":1,"unique_ips":6},{"no":122343637,"now":"04\/22\/17(Sat)19:16:55","name":"Anonymous","com":"<span class=\"quote\">&gt;Automation is a great excuse for assholery \u2013 after all, it\u2019s just numbers, and the magic of \u201cbig data\u201d can provide plausible deniability for prejudice. Machine learning, as the technologist Maciej Ceg\u0142owski observed, can function in this way as \u201cmoney laundering\u201d for bias.<\/span><br><br><span class=\"quote\">&gt;This is a problem, and it will become a bigger problem unless we take active measures to fix it. We are moving into an era when \u201csmart\u201d machines will have more and more influence on our lives. The moral economy of machines is not subject to oversight in the way that human bureaucracies are. Last year Microsoft created a chatbot, Tay, which could \u201clearn\u201d and develop as it engaged with users on social media. Within hours it had pledged allegiance to Hitler and started repeating \u201calt-right\u201d slogans \u2013 which is what happens when you give Twitter a baby to raise. Less intentional but equally awkward instances of robotic intolerance keep cropping up, as when one Google image search using technology \u201ctrained\u201d to recognise faces based on images of Caucasians included African-American people among its search results for gorillas.<\/span><br><br><span class=\"quote\">&gt;These, however, are only the most egregious examples. Others \u2013 ones we might not notice on a daily basis \u2013 are less likely to be spotted and fixed. As more of the decisions affecting our daily lives are handed over to automatons, subtler and more insidious shifts in the way we experience technology, from our dealings with banks and business to our online social lives, will continue to be based on the baked-in bigotries of the past \u2013 unless we take steps to change that trend.<\/span>","time":1492903015,"resto":122343469,"id":"ZmgSzrle","country":"US","country_name":"United States"},{"no":122343805,"now":"04\/22\/17(Sat)19:18:12","name":"Anonymous","com":"You don&#039;t have to shitpost badly to discuss an article nigger. Just post it and an archive. Fuck.","time":1492903092,"resto":122343469,"id":"X2bq0jSR","country":"US","country_name":"United States"},{"no":122344011,"now":"04\/22\/17(Sat)19:19:50","name":"Anonymous","com":"<a href=\"#p122343805\" class=\"quotelink\">&gt;&gt;122343805<\/a><br>http:\/\/archive.is\/nOWfk<br><br>Nigger every time I do that the post sinks like a rock. Bait is literally the only way to get comments outside of happening thread.","time":1492903190,"resto":122343469,"id":"ZmgSzrle","country":"US","country_name":"United States"},{"no":122344108,"now":"04\/22\/17(Sat)19:20:38","name":"Anonymous","com":"<a href=\"#p122344011\" class=\"quotelink\">&gt;&gt;122344011<\/a><br>Be the change you want to see.","time":1492903238,"resto":122343469,"id":"X2bq0jSR","country":"US","country_name":"United States"},{"no":122344207,"now":"04\/22\/17(Sat)19:21:29","name":"Anonymous","com":"Just wait until I&#039;ve finished building the Niggerkiller9000.","time":1492903289,"resto":122343469,"id":"WZXBfitF","country":"GB","country_name":"United Kingdom"},{"no":122344428,"now":"04\/22\/17(Sat)19:23:09","name":"Anonymous","com":"Wow, she really doesn&#039;t understand programming does she","time":1492903389,"resto":122343469,"id":"juxHEGWu","country":"US","country_name":"United States"},{"no":122344454,"now":"04\/22\/17(Sat)19:23:19","name":"Anonymous","com":"<a href=\"#p122343469\" class=\"quotelink\">&gt;&gt;122343469<\/a><br><span class=\"quote\">&gt;Robots use pure facts and statistics to determine Dindus are subhuman<\/span><br>kek","time":1492903399,"resto":122343469,"id":"PfAFThLh","country":"US","country_name":"United States"},{"no":122345289,"now":"04\/22\/17(Sat)19:30:10","name":"Anonymous","com":"<span class=\"quote\">&gt; using technology \u201ctrained\u201d to recognise faces based on images of Caucasians included African-American people among its search results for gorillas.<\/span><br><br>It wasn&#039;t wrong, you know.","filename":"150501999-5147155f-d1ee-4f70-9168-6af84f165f63","ext":".jpg","w":560,"h":315,"tn_w":125,"tn_h":70,"tim":1492903810471,"time":1492903810,"md5":"92cXYmj60Q\/Nl0wihzuzuw==","fsize":26015,"resto":122343469,"id":"9mO1InsN","country":"NL","country_name":"Netherlands"}]}